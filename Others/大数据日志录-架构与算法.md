---
title: 大数据日知录-架构与算法
date: 2017-11-8
tags: [大数据, 概述, Big Data]
---

# 大数据日知录-架构与算法

## Chapter0 大数据概述
* 背景:随着互联网技术的飞速发展，硬件存储成本的下降，使得大量数据的收集存储成为可能，因为数据呈现井喷式爆炸增长。
* 大数据特点(4V)
    * Volume: 大容量
    * Variety: 形式多样(如存在形式，结构化/半结构化/非结构化数据，数据类型，如文本数据/图像/视频等,对应产生了更具扩展性的Nosql)
    * Velocity: 高速率，实时性/时效性
    * Value: 价值密度较低(通常需要挖掘/分析，提取有效信息)
* 大数据处理层次结构:
    * 数据收集: 来源于传感器，移动设备，企业数据，互联网等，通常需要去噪抽取为结构化或半结构化数据
    * 数据存储管理: 分布式文件系统HDFS,NoSQl(图数据库，列式数据库HBase,)
    * 数据挖掘分析: 机器学习，时空序列分析，统计分析等
    * 数据可视化展示: 形象易懂

## Chapter1 数据分片与路由
* 概述：由于数据的爆炸式增长，传统的通过增加单机性能的纵向扩展，已不能满足大数据存储的需求，进而通过增加机器数据，进行分布式的横向扩展。因此往往需要对大数据进行数据分片，在数据查找时如何确定数据存储的位置便需要数据路由。
### 数据分片：
* 抽象模型：通常分为两级映射:1,key-partition：数据key值到数据分片的映射；2.partition-machine:数据分片到机器的映射；
* 哈希分片:主要通过哈希函数确定数据分片，仅支持点查询，不支持范围查询；如Dynamo,Cassandra,Riak,Voldmort,Membase等大多数KV存储系统
    * Round Robin: 即对key进行取模，hash(key)mod K(K通常为物理机器数量)，由于哈希函数与物理机器数有关，所以系统通常难以扩展。
    * 虚拟桶:所有数据记录通过哈希函数映射到对应的虚拟桶，然后通过查表将虚拟桶映射到物理机(通常为多对一关系)，相比之下更加灵活
    * 一致性哈希:
        * 分布式哈希表DHT是一种技术概念，是常用于P2P(Peer-to-Peer,对等联网)网络和分布式存储，一致性哈希为其一种实现
        * 一致性性哈希算法(Chord,和弦系统中的一致性哈希算法):将哈希数值空间按照大小组成一个首尾相接的环状序列，不同的机器可根据其端口号或Ip等信息映射到环状序列中的不同节点，从而每台机器负责存储维护一段有序哈希空间内的数据，实现数据分片。
            * 路由: 
                * 每个机器配有路由表(通常包含m项，m为哈希空间二进制数值比特位长度，其中第i项路由信息代表距离当前节点为2^i的哈希空间数值所在的机器节点编号),
                * 查找过程如下:
                    * 节点i发起查找，后继为节点s,H(key)=j,若i\<j\<=s说明key对应的数据位于节点s,节点i向s发送消息请求查询key的对应数据值，并返回给节点i，
                    * 否则节点i查询自身的路由表，找到小于j的最大编号n(若均大于j则选择第m-1项路由信息的编号作为n),节点i向节点n发送消息请求查询，此时节点n作为i进行以上步骤，以此循环；
            * 增加新节点:
                * 首先根据想要插入的位置，确定其后继节点，将new节点的后继指向s,前继节点置为空；
                * 然后稳定性检测(所有节点均会周期性自动完成)，完成前驱和后继节点的更新以及数据前移；
                * 此外整个过程的完成还需要等待其他节点的稳定性检测，同时每个节点需要周期性的检查其路由表，对路由表内每一项(k=2^i)加上本机节点编号执行路由算法，得到每个路由项经过查询获得的机器节点，与当前内容不同则更新。
            * 删除已有节点:
                * 正常删除:在离开前通知相应节点更新其前驱和后继节点，以及将本身持有的数据迁移到后继节点上，同时其他节点更新各自路由表
                * 异常删除(往往由于机器故障)，为避免数据丢失可利用数据副本
            * 稳定性检测步骤:
                * 1.节点i向其后继节点s询问其前驱节点p，若后继s的前驱节点p为i本身,则调到步骤4，否则调到步骤2
                * 2.如果节点p介于i和s之间，则i节点记录下p为其后继节点
                * 3.选择i当前的后继节点作为x(可能为s也可能为p),如果x的前驱节点为空，或者i为x和他的前驱节点之间，则i给x发消息指示i就是x的前驱节点，此时x将其前驱节点设为i
                * 4.x(p)节点将部分数据迁移到i节点(即哈希值小于等于i的数据)
            * 虚拟节点:考虑到机器差异，负载均衡等问题，可将一个物理机器虚拟成多个虚拟节点
* 范围分片：通常对key值进行排序，然后划分范围确定数据分片，通常通过映射表(可利用B+树)将数据分片映射到物理机器，对于数据分片在物理机上的管理可利用LSM树。如PNUTS,Azure,Bigtable

## 数据复制与一致性
* 数据复制：为提高系统的容错性和高可用性，往往需要对数据进行冗余存储，同时数据副本可以大大提高数据并发读取的效率。同时数据多副本也会带来并发更新时的一致性维护问题
### 基本原则
* CAP原则(对于同一个系统三者不可兼得，至多只能保证其中两个，应根据实际往往在AP/CP间进行取舍选择)
    * Consistency强一致性:在分布式同一数据多副本的情况下，数据的更新操作随时保持一致(对外如单份数据一样，提供一致性视图)
    * Availability可用性:客户端在任何时刻对大规模数据系统的读写操作应保证在有限时间内完成
    * Partition Tolerance分区容忍性:在大规模分布式数据系统中，网络分区(即分区间机器无法进行网络通信)必然发生，在这种情况下应保证系统可以正常工作
> 注:通常网络分区出现的概率很小，因此在一定程度上我们可以同时保证CA;其次在AC间取舍时,我们可分别考虑中不同的子系统，或针对系统的不同运行时期进行细粒度的策略选择，差异处理；其次CAP原则并非绝对，应将其看作连续变量，可根据实际情况，决定在多大程度上进行保证/舍弃；
* ACID原则(通常数据库系统因此获得高可靠性和强一致性)：
    * Atomicity原子性:即一个事物要么全部执行，要么全部不执行
    * Consistency一致性:在事物的开始和结束总是满足一致性约束条件
    * Isolation事物独立:若多个事物同时执行，彼此之间互不知晓对方的存在，互不影响；即事物之间需要序列化执行
    * Durability持久性:指事物运行成功后对系统状态的更新是永久的，不会无缘故的回滚撤销；
* BASE原则(通常适用于大数据环境下的云存储和NoSql，通过牺牲一致性来获得高可用性)：
    * Basic Available基本可用，允许偶尔失败
    * Soft State软(柔性)状态:指不要求数据状态在任意时刻都要求同步
    * Eventual Consistency最终一致性：符合弱一致性，要求在给定的时间窗口内数据会达到一致状态
> 注:目前NoSql和云存储系统发展过程中正在向逐步提供局部ACID特性发展，同时全局符合BASE原则，发挥两者优势，在两者间建立平衡
* 总结: CAP中的C指强一致性，是ACID中C的一个子集；当网络分区发生时,ACID的事物独立(I)只能在某个分区执行，多个分区会各自进行持久化(D),网络分区解决后需要解决冲突，使整个系统重新达到一致性状态；
* 幂等行性:调用方反复执行同一操作与只正确执行一次操作，效果相同；
### 一致性模型分类
* 强一致性:当某个数据被更新后，其后续所有的观察者应立即感知到其更新值，并以此为基础进行读/写;
* 弱一致性:所有不满足强一致性的情况
    * 最终一致性：在一个时间片段(不一致窗口)后保证所有观察者感知到次更新值
        * 因果一致性:实用与进程之间有因果关系的情形，在某个进程更新了数据值后，会通过消息通知其因果进程数值已改变，从而具有因果关系的进程感知到数据的更新，而对其他进程在不一致窗口内仍无法保证感知到数据的更新；
            * 读你所写一致性:进程更新数据值后保证自身立即感知到数据的更新；
                * 会话一致性:当进程通过会话与数据库连接时，在同一个会话内保证"读你所写"一致性，而在不一致窗口内，若会话终止进程可能无法感知自身的更新操作
        * 单调写一致性:保证在某个进程之后对数据进行读取的其他进程看到的数据版本不能比此进程看到的更老；
        * 单调读一致性:保证多次写操作的序列化
### 副本更新策略
* 同时更新:若不通过一致性协议，对于同一时刻的接收的更新消息无法确定其执行顺序，造成潜在的数据不一致问题；若通过一致性协议预先处理，可确定不同更新操作的执行顺序，保证数据一致性，但一般会增加请求延时
* 主从式更新：副本中存在一个主副本，其他为从副本，所有对这个数据的更新操作首先提交到主副本，再由主副本通知从副本进行数据更新，有主副本决定不同更新操作的顺序
    * 同步方式:主副本等待所有从副本更新完成之后才确认更新操作完成。可保证数据的强一致性，但请求延时较大。
    * 异步方式:主副本在通知从副本更新完成之前即可确认更新操作。可能出现数据一致性问题(一般会在可靠位置保存更新操作记录)
        * 所有读请求都通过主副本来响应(其他副本将读请求转发到主副本处理)，可以保证数据的一致性，但同时也会增加请求延时(如Chubby)
        * 数据的所有副本都可以自己响应读请求，会出现数据的不一致的问题，但同时会降低请求延时(如PNUTS,Zookeeper)
    * 混合方式:主副本首先同步更新部分从副本数据即可确认更新完成，然后其他从副本数据异步更新完成（如Kafka）
        * 若读操作至少从一个同步更新的节点读出，则一致性获得保证,如RWN协议的R+W>N模式，相比同步方式请求延时相对较小,否则会出现读不一致问题，如R+W<N
* 任意节点更新：数据更新请求可能发送给多副本中的任意一个节点，然后由这个节点通知其他节点进行更新。相比"主从式更新"副本中的任意一个节点都可能响应更新请求(因此有可能同一时间对同一数据发出更新请求，但是由不同副本进行响应)，类似于"主从式更新"，对于通知的方式可能有同步/异步两种方式
> 注:Dynamo/Cassandra/Riak采用了主从式的混合方式，当主副本发生故障式则启动任意节点更新策略
### 一致性协议
#### 两阶段提交协议2PC:保证分布式事务中，要么所有参与进程都提交事务，要么都取消事务(如可保证数据更新操作的多副本一致性)。更多用来实现数据更新操作的原子性(如Raft协议),可将参与者分为：唯一的协调者,众多的参与者
    * 表决阶段(Voting)：协调者(初始为INIT状态，当接到系统的commit消息后)向所有参与者发送一个VOTE_REQUEST消息(进入WAIT状态)，当参与者(初始为INIT状态)接收到该消息，向协调者回应VOTE_COMMIT消息(进入READY状态)，表示自己已做好提交准备，否则回应一个VOTE_ABORT消息，表示自己目前尚无提交事务的可能
    * 提交阶段(Commit):协调者收集汇总缩所有参与者的表决信息，如果所有参与者均已准备好提交事务(回应为VOTE_COMMIT)，则协调者决定最终事务可提交，并向所有参与者发送GOLBAL_COMMIT消息，通知参与者进行本地提交；否则如果有任意一个参与者没有准备好(返回VOTE_ABORT)，则协调者决定取消事务，并向所有的参与者发送GLOBAL_ABORT消息，通知其取消事务；然后参与者根据收到的消息进行本地操作
* 如果一个协议包含阻塞态(如2PC中协调者的WAIT状态和参与者的INIT和READY状态)，那么明显是一个脆弱的系统,很可能整个系统处于长时间等待。为解决此问题，提出了以下方法：
    * 超时判断机制(解决协调者WAIT和参与者INIT)：WAIT状态的协调者在指定时间内未能收集全部参与者的回应信息，则向全部参与者多播GLOBAL_ABORT消息；如果INIT状态的参与者在规定时间内为收到协调者的VOTE-request消息，则可以简单的中止本地事务，并向协调者发送VOTE-request消息
    * 参与者互询机制(解决参与者READY):READY状态的参与者在规定时间内未收到消息，可以询问另外的参与者：
        * 若另外的参与者处于COMMIT状态，则等待的参与者可以提交本地事务并进入COMMIT状态
        * 若另外的参与者处于ABORT状态，则等待的参与者可以取消本地事务并进入ABORT状态
        * 若另外的参与者处于INIT状态，此时等待的参与者可取消本地事务并进入ABORT状态
        * 若另外的参与者处于READY状态，则可以继续询问另外的参与者(若所有的参与者均处于READY状态，则只能长时间等待协调者重新启动或故障恢复) 
* 三阶段提交协议(3PC，为解决2PC协议的长时阻塞):将2PC的提交阶段再次细分为预提交和提交两个阶段，通过引入预提交，从而使协调者和参与者:
    * 没有可以既可以直接转换到COMMIT又可以直接到ABORT状态的单独状态；
    * 不存在既不能做出决定，而且可从它直接转换到COMMIT状态的状态(PRECOMMIT已经做出决定)；
#### 向量时钟(Vector Clock)：
* 向量时钟通过将时间戳和事件绑定，在分布式环境在生成事件偏序关系(代表事件发生的先后顺序导致的事件间的因果依赖关系)的算法
* 每个进程记录初始值为0整数向量时钟VCi(1....n),其中第j项代表该进程i看到的进程j的逻辑时钟，并以如下规则更新:
    * 当进程i产生发送消息或接收消息或进程内部事件时，其将自己的向量时钟自己对应的位加一，即VCi[i]+=1;
    * 当进程i发送消息m时，同时将自己的向量时钟(mVCi)和消息m一起发送出去
    * 当进程j接收到进程i发送过来的消息时，更新自己的向量时钟的每一位数值:VCj[x]=max(VCj[x],mVCi[x]),其中x=1...n
* 若事件1的时钟向量的每个维度的数值均小于等于事件2的对应位置的数值且至少有一位小于，那么称事件1是事件2的原因，事件2是事件1的结果(指两者逻辑上因果影响关系)；
* 应用:Dynamo中使用向量时钟进行数据版本管理，配合RWN协议共同完成数据一致性维护，Risk类似
#### RWN协议:
* 通过对分布式环境下多备份数据读/写进行约束配置来达到数据一致性，其中:N为数据的备份数量;R为一次成功的读操作至少要有R个备份被成功读取;W为一次成功的更新操作至少要有W份备份被成功写入
* R+W>N则保证读写副本一定存在交集，保证数据的强一致性(需要配置向量时钟判断数据的最新值)，R/W设置的越大，系统延迟越大，可根据实际的需求对R/W进行灵活配置
#### Paxos协议:
* 副本状态机模型(采用Log副本机制):每台机器各自保存一份Log副本和内部状态机，Log中顺序记载客户端发来的操作指令，并通过一致性模块保持各机器的Log副本内容一致，机器将依次执行Log中的指令更新自己的内部状态机，从而所有机器的内部状态得到一致，在实现时通常考虑:
    * 安全性(非拜占庭模型下，状态机从不返回错误结果，多个提议中只会有一个被选中)
    * 可用性保证，容忍少数机器发生故障
    * 大多数状态机维护Log一致性即可快速通知客户端操作成功
* 在Paxos中并行进程分为倡议者(提出提议)，接收者(投票表决选出唯一的提议),学习者(无投票权，可获知最终选举产生的唯一提议)
* 异步通信环境下的非拜占庭模型:
    * 并发进程的行为可以任意速度执行，允许运行失败，失败后可重试
    * 并发进程之间通过异步方式相互通信，通信时间可任意长，传输过程信息可能丢失，允许消息重发，多重消息顺序可任意，但不允许消息被篡改(可通过内容完整性检测判断)，Paxos等很多一致性协议均基于非拜占庭模型(满足该条件可就多个并行进程的不同操作(提议)达成一致)
* Paxos一致性协议过程(阶段一):
    * 1.1倡议者选择倡议编号n，并向半数以上的接收者发送Prepare请求，并携带倡议编号n
    * 1.2接受者接收到带有倡议编号n的Prepare请求：
        * 若n均大于此接受者以前响应过的任何其他Prepare请求的倡议编号,则接受者响应该倡议者(并承诺不会响应之后接收到的倡议编号小于n的请求)，否则无回应
        * 另外，若接受者曾经响应过2.2阶段的Accept请求，则将所有响应的Accept请求中倡议编号最大的倡议内容发送给倡议者(倡议内容包括Accept请求中的倡议编号以及其倡议值)
* Paxos一致性协议过程(阶段二):
    * 2.1若倡议者接收到大对数接受者关于倡议编号为n的Prepare请求的响应，则向这些接受者发送Accept请求(弱在接受者返回了之间曾经接收的倡议内容，则接受者回应的倡议内容中选择倡议编号最大的倡议内容对应的倡议值，否则倡议值可任意)
    * 若接受者收到倡议编号为n的Accept请求，若此接受者在此期间没有响应过倡议编号大于n的Prepare请求，则接受此请求
    * 最终将选出的唯一倡议者通知所有的学习者
* Paxos协议又可分为多Paxos协议(对应的Log内容中的多个位置的操作命令序列一致)和单Paxos协议(对应的Log内容中的某个位置的操作命令序列一致)

#### Raft协议
* Raft协议将整个一致性协议划分为领导者选举，Log复制和安全性3个子问题，并将Paxos的P2P模式改为master-slave模式
* 在Raft协议中，集群中的机器只能处于三种状态:
    * Leader:正常情况只有一个，其负责响应所有的客户端请求
    * Follower:只被动接受RPC消息
    * Candidate：是Follower状态机器准备发起新的Leader选举需要转换到的中间状态
* Raft协议将整个系统执行时间划分为若干个不同时间间隔长度的时间片长度(Term,以递增数字标识)构成的序列，在Tearm开始时会选出一个该时间片内的Leader；
* 领导者选举:
    * 系统启动时，所有机器处于Follower状态，等待接收RPC消息，若Follower在选举超时时间后仍没有接收到Leader的心跳信息，则引发其启动新领导的选举过程，其中Leader通过向其他机器周期性的发送心跳信息保持其Leader地位
    * 在开始选举前，Follower增加其Term编号并转入Candidate状态，然后向集群内的其他所有机器发出RequestVote RPC消息，之后一直处于Candidate状态，直到:
        * 它赢得了本次选举(接收到大多数其他具有相同Term的投票)，然后通过向其他机器发送RPC心跳宣告其Leader地位
        * 另外一个机器宣称并确认自己是新的Leader，若此Leader的Term编号大于等于自身Term编号，则承认此Leader有效，并转为Follower状态。否则拒绝承认
        * 时间超时，仍没有Leader产生(可能同一时刻多个Follower转为Candidate状态，导致选票分流)，超时后增加自身Term编号进入下一轮选举(各机器超时时间随机)
* Log复制:
    * Leader接收到客户端的操作命令后，将其追加到Log尾部(附带Term编号)，然后向集群内的其他服务器发出AppendEntries RPC请求，引发其他服务器复制新的操作命令，安全复制完成后，Leader将这个操作命令应用到内部状态机，并将执行结果返回给客户端
    * 此外有一个全局的索引指示操作在Log中的顺序编号，若两个操作记录具有相同的全局索引编号和相同的Term编号，则两个操作命令相同，且这个操作记录之前的所有前驱操作记录都完全相同
* 安全性:
    * 只有其Log包含了所有已经提交的操作命令的机器才可以被选为新的Leader
    * 对于新Leader只有它自己已经提交过当前Term的操作命令才被认为是真正提交

## 大数据常用算法与数据结构
### 布隆过滤器BF(Bloom Filter):
* 概述:采用二进制向量数据结构，空间效率极高,常用于判断是一个元素在大规模集合中是否存在(存在误判的问题，但不会漏判)
* 利用长度为m的位数组(初始全部置为0)储存集合信息,同时实用k个相互独立的哈希函数将数据映射到位数组空间(对于数据a,Hi(a)=x,则将位数组的x位置为1，对于每个数据分别利用k个哈希函数进行计算可将位数组的w位置为1，w<=k)，当查询数据b时，实用相同的k个哈希函数进行计算，若对应的位均为1则集合包含该数据，有一个位为0则不包含。
* 误判率主要与集合大小n,哈希函数个数k，位数组大小m有关:P约等于（1-e^-(kn/m)）^k;在n和m确定时k=(m/n)ln2时误判率P达到最低；在集合大小n确定时，m=-(nlnP/(ln2)^2)
 * 改进(满足集合动态变化的需求)：Counting Bloom Filter将位数组的每一位扩展为n位，当Hi(a)=x时数组对应的x位加一(同时在删除一个元素时，映射后对应的位减一)
 * 应用:实用于数据量极大且容忍误判的情况，数据库中的Bloom Jion加速两个大小差异巨大的表的Jion过程；在Bigtable中数据读取也用到BF,此外还有Cassandra,MillWheel
 ### SkipList
 * 一种可代替平衡树的数据结构(大多数情况下可实现O(logn)的增删查改时间复杂度)，依靠随机生成数以一定概率保持数据的平衡分布，适用于有序存储，如LevelDB中的MemTable,Redis中的Sorted Set,Lucene中的倒排列表的查找都用到这种数据结构
 * 为链表中的部分节点增加更多的指针指向其后面的不同距离节点，并且在插入节点时随机决定该节点有多少指向后续节点的指针(称为该节点的Level,表头具有MaxLevel层级)
* 查找:从首节点的最高层级节点开始查找，若node-key小于search-key，指针后移，若等于则返回结果。若大于则指针移向下一层级，到达尾部还没找到则查找失败
* 插入:首先类似用类似查找的方法找到要插入的位置(第一个node-key>search-key的的节点前，若找到key相同节点则直接更新该节点值)；然后保存每一层的后继指针；然后生成插入新节点并调整指针；
* 删除：类似插入，查找要删除的节点，保存指针，删除节点后重新调整指针
### LSM树
* 本质是将大量的随机读写转换为批量的序列写，使用于对写操作效率有高要求的场景(但读效率有所降低，利用Bloom Filter优化)
* 一般实现原理(以LevelDB为例):
* 构成LevalDB的静态结构包含6个主要部分:
    * 内存中:MemTable(采用SkipList结构); Immutable Memtable(不可更改，可读取)
    * 磁盘上:Current文件;manifest文件;log文件(用于系统崩溃后恢复);SSTable文件(多个，以不同Leval分层)
* 当用户写入一条KV记录时，LevalDB先将操作写入log文件(磁盘顺序写)，成功后将该记录插入Memtable中，从而完成写入操作；
* 当Memtable中插入的数据占用内存到一个界限后,LevelDB会生成新的Log文件和Memtable,原来的Memtable成为Immutable Memtable，此后的写操作存入新的long文件和Memtable。LevelDB后台调度系统会将Immutable Memtable的数据导入到磁盘，(经过Compaction)形成一个新的SStable文件
* SSTable中主键有序，除Level0外两个SSTable(.sst)文件主键范围不存在重叠。
* manifest记载SSTable各个文件的管理信息(如属于哪个Level,文件名，最小key/最大key)
* Current用来当前的manifest文件名(在SSTable的Compaction的过程中会生成新的manifest)
* Compaction机制(与Bigtable基本一致)，分为三种:
    * minor Compaction:将Memtable中的数据导出到SSTable文件中，根据key值从小到大依次写入Level0,写完后建立文件的index数据(不涉及删除)
    * major Compaction:合并不同层级的SSTable文件，当Level下的SSTable文件数目达到一定设置值时，LevelDB会从这个Level(>0)的SSTable中选择一个文件，和高一层级(Level+1)的(与此文件存在key重叠)SSTable文件合并;对于Level0，当选择一个文件后，会找到Level0中所有与此文件的key范围存在重叠的文件，然后与上层文件合并，关于合并过程采用多路归并排序，写入一个新文件(并删除不必要的记录)，此后删除原来参与合并的旧文件
    * full Compaction:将所有SSTable进行合并(LevelDB不包含)
* 应用:如Bigtable的单机数据引擎，基于Flash的海量存储系统SILT，内存数据库RAMCloud，此外Cassandra,LevelDB也采用了LSM树

### Merkle哈希树：
* 起初用户高效Lamport签名验证，在分布式中用来在海量数据下快速定位少量变化的数据内容
* 其叶子节点为每个数据或数据块对应的哈希值，其中间节点为对其所有子节点再次进行哈希运算后的值，依次向上直至根节点，当某个数据发生变化时，其对应的叶子节点的哈希值发生变化，该叶子节点的父节点对应的哈希值也随之发生变化，向上传递直至根节点随之发生变化，通过哈希值比较，从而可以在O(logn)的时间复杂度内快速定位发生变化的数据。
* 应用:
* Dynamo结合Merkle树(用来快速定位多副本中不一致内容)和Gossip协议来对副本数据进行同步；
* 比特币中使用Merkle树来对交易进行验证(根据交易信息的哈希中定位在最长链表中的位置依次向上对比哈希值，能够到达根节点说明合法)

#### Snappy与LZSS算法
* Snappy是一种基于LZ77(更准确的是LZSS)高效的数据压缩和解压缩算法库，在合理的压缩率的基础上追求尽可能快的压缩速度，在Bigtable,Mapreduce,PRC中都能使用，其中Hadoop,HBase,Cassandra,Avro都用到此算法库；
> 数据压缩算法本质上是通过增加CPU计算时间成本来换取较小的存储成本，以及网路和I/O传输成本。
* LZSS算法:LZSS是一种动态词典编码(与静态词典不同,从文本中自动导出词典，后续文本中的词用它在词典中表示位置的号码代替)
    * 采用基于滑动窗口缓存技术和前向缓冲区,由滑动窗口(包含若干已经处理过的字符)内的文本构造词典，前向缓冲区包含若干待处理的字符，将前向缓冲区中的第一个字符与滑动窗口中的字符进行匹配，若匹配长度超过最小匹配长度限制，则将匹配字符串作为(指针(位置)，长度，后续字符)输出,否则将该字符移入滑动窗口，并将窗口内最久的字符移除(LZ77最小匹配长度为1)
    * 关于在滑动窗口中找出最长匹配字符串:将滑动窗口内各种长度的字符串片段存入哈希表，并记录其位置，在匹配时首先在哈希表中查找该字符串
* Snappy算法遵循LZSS算法压缩编码与解码方案，最小匹配长度限制为4，其滑动窗口每次后移的长度也为4.其哈希表内存储的字符串长度也为4，此外其将文件分为32KB的数据片分别压缩
#### Cuckoo哈希
* 有效解决哈希冲突问题，可在O(1)的时间复杂度内查找删除数据，在常数时间内插入数据，大约50%的哈希空间利用率。
* 基本原理:Cuckoo哈希使用两个不同的哈希函数，对于一个数据x，分析利用两个哈希函数计算其哈希值，若其中一个对应位置为空则将其放入，否则随机选择一个位置，将原来的数据y踢出,对于踢出的数据y,将其和原来的数据x一样处理，直至所有的数据都找到位置(通常会设置一个最大替换次数)，此外可以增加哈希函数的个数提高空间利用率，也可以增加每个桶可容纳的数据个数，减少替换次数。
* 应用:SILT(small Index Large Table)存储系统利用Cuckoo哈希变体来作为外存索引，是一种基于FLash的高效利用内存的高性能KV存储系统，其将KV数据顺序追加到存储在Flash上的Log文件中，并在内存中利用部分主键Cuckoo哈希建立索引:
    * 其哈希桶里存在主键Key的标签，节省空间
    * 对于两个哈希函数对应的桶1和桶2，两个哈希函数互相将对方的哈希空间位置作为自己的标签

# 集群资源管理与调度
* 背景:互联网公司为适应各种需求需要各种不同类型的计算系统(如实时计算的挖掘系统，适合交互查询场景的系统，批处理系统等)，面对各种类型的计算系统与框架，高效的资源管理与调度变得越来越重要，相比之下，传统的静态资源划分资源利用率较低，已经不能满足需求。因此设计出好的资源管理调度策略和方法。使得整个集群的大量资源在能够实现更高资源利用率的同时加快所有计算任务的整体完成速度成为集群资源管理与调度系统的核心目标
* 趋势:在集群硬件层之上抽象出一个功能独立的集群资源管理系统，将所有可用资源当作一个整体来进行管理，并对其所有计算任务提供统一的资源管理与调度框架和接口，计算任务按需向其申请资源，使用完毕释放给资源管理系统,优点:
    * 资源利用率高:根据不同计算任务即时动态分配资源
    * 增加数据共享能力:所有资源对所有计算任务可用，避免了资源(数据)的冗余
    * 支持多类型多版本计算框架:方便互联网的实际业务需求(快速的版本迭代更新)、
## 资源管理抽象模型
* 基于现有的资源管理与调度系统(YARN,Mesos,Corona,Quincy等)总结得出:资源管理的概念模型和统一架构;
### 概念模型
* 概念模型:将集群中的各种资源通过一定策略分配给用户提交到系统的各种任务。主要强调三要素:
    * 资源组织模型(常见的资源包括内存,CPU,网络资源,磁盘I/O 4类)，常见的资源组织方式是将资源组织成多层级队列的方式(如Facebook的Corona将资源组织模型建为"all resource->group->pool"三级队列结构)，此外还有平级多队列以及单队列模型。
    * 调度策略(包括FIFO,公平调度,能力调度，延迟调度等),以一定的方式将资源分配给待处理的任务
    * 任务组织模型:将提交的任务组织起来，方便后续的资源分配。如Hadoop1.0将任务按照平级多队列组织，Hadoop2.0中的能力调度器增加了层级队列的树形队列结构.Quincy(微软Dryad系统的资源管理系统)采取全局队列，机架队列以及节点队列的树形队列结构作为任务组织模型
### 通用架构
* 集群中每台机器上会配置节点管理器，其主要职责是不断地向资源收集器汇报目前本机资源使用情况，并负责容器的管理工作(当某个任务被分配到本节点时,节点管理器负责将其纳入某个容器执行并对该容器进行资源隔离，以避免不同容器内任务的相互干扰)。
* 通用调度器由资源收集器和资源调度策略构成，同时管理资源池和工作队列数据数据结构。
    * 资源收集器不断向集群内各个节点收集和更新资源状态信息，并将其最新状态反映到资源池中，资源池列出了目前可用的系统资源。
    * 资源调度策略规定了如何将资源池中的可用资源分配给工作队列，常见的包括FIFO,公平调度，能力调度策略等。在实际中，资源调度策略往往是可插拔的

## 调度系统设计的基本问题：
* 资源异质性与工作负载异质性:如集群机器硬件配置差异；各种服务和功能差异，对资源需求的差异
* 数据局部性:大数据场景基本原则为将计算任务推送到数据所在地(减少网络传输量)
    * 在资源管理与调度语境下，有3种类型的数据局部性：节点局部性(无需网络传输)，机架局部性(同机架节点间网络传输速度较高)，全局局部性；
* 抢占式调度与非抢占式调度:适用于多用户多任务场景下
    * 抢占式调度:对于某个计算任务，如果资源不足或不同任务竞争同一资源，调度系统可以从比当前计算任务优先级更低的其他任务种获取已分配的资源，而被抢占的任务需要出让资源停止计算，可能后续继续重新申请资源来完成后续计算，有时甚至需要废弃已完成的计算任务。如Omega调度系统
    * 非抢占式:只允许从空闲资源中进行分配，如果当前空闲资源不足，则必须等待其他任务释放资源。如Mesos中的资源调度
* 资源分配粒度:
    * 大数据场景下计算任务往往由两层结构构成:作业级和任务级，其中一个作业由多个并发任务构成，任务间的依赖关系有有向无环图(DAG，如典型的Mapreduce为特殊的DAG关系)
    * 关于资源分配粒度问题：
        * 一种极端情况是将作业的所有所需资源一次性分配完成(常被称为"群体分配"或"全分或不分")，如MPI任务；
        * 另一种是采取增量满足式分配策略，即对于某个作业来说，只要分配部分资源就能启动一些任务，随着空闲资源不断出现，可以逐步增量式分配给作业其他任务以维持作业不断向后推进。如以MapReduce为代表的批处理任务等
        * 有一种特殊的增量满足式分配策略被称为"资源储备"策略，指只有分配到一定量的资源后作业才启动，但在为获得足够资源之前，作业可以先持有目前已分配的资源，并等待其他作业释放资源，从而不断持有储备累积，直到达到最低标准后开始运行(在启动钱，已持有的资源处于闲置状态)
* 饿死与死锁问题：常因为资源管理与调度策略设计不当，
    * 某个任务持续长时间无法获得开始执行所需的资源，一直等待出现饿死(如优先级低的任务等)。
    * 若系统的中的作业长时间无法继续推进，导致系统无法正常运行，从而导致调度系统进入死锁状态(如“资源储备”策略中，所有作业互相等待对方释放资源)
    * 调度系统出现死锁必然表现为某些作业出现饿死，但计算任务处于饿死未必意味着调度系统处于死锁状态。合理的资源管理与调度策略应避免出现任务饿死和系统死锁的问题
* 资源隔离方法:相比hadoop1.0的Map和Reduce槽的粗粒度资源分配方式，无论是YARN还是Mesos都采用了将各种资源(CPU,内存,网络带宽和I/O带宽)封装在容器中的细粒度资源分配
    * 整个分布式资源管理系统封装了众多的资源容器，为了避免不同任务之间互相干扰，需提供容器间的资源隔离方法，目前常用的手段是Linux容器(LXC，如YARN,Mesos):
        * LXC是一种轻量级内核虚拟化技术，可以用来进行资源和进程运行的隔离，可通过LXC在一台物理主机上隔离出多个相互隔离的容器(依赖于内核的cgroups子系统，一个内核提供的基于进程组的资源管理框架，可为特定进程组限定可以使用的资源)，如Google的大规模资源系统Borg和Omega采用了其开源的的Linux容器lmctfy系统，类似的还有Cloud Foundry的Warden

## 资源管理与调度系统范型        
