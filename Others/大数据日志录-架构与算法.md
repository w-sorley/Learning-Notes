---
title: 大数据日知录-架构与算法
date: 2017-11-8
tags: [大数据, 概述, Big Data]
---

# 大数据日知录-架构与算法

## Chapter0 大数据概述
* 背景:随着互联网技术的飞速发展，硬件存储成本的下降，使得大量数据的收集存储成为可能，因为数据呈现井喷式爆炸增长。
* 大数据特点(4V)
    * Volume: 大容量
    * Variety: 形式多样(如存在形式，结构化/半结构化/非结构化数据，数据类型，如文本数据/图像/视频等,对应产生了更具扩展性的Nosql)
    * Velocity: 高速率，实时性/时效性
    * Value: 价值密度较低(通常需要挖掘/分析，提取有效信息)
* 大数据处理层次结构:
    * 数据收集: 来源于传感器，移动设备，企业数据，互联网等，通常需要去噪抽取为结构化或半结构化数据
    * 数据存储管理: 分布式文件系统HDFS,NoSQl(图数据库，列式数据库HBase,)
    * 数据挖掘分析: 机器学习，时空序列分析，统计分析等
    * 数据可视化展示: 形象易懂

## Chapter1 数据分片与路由
* 概述：由于数据的爆炸式增长，传统的通过增加单机性能的纵向扩展，已不能满足大数据存储的需求，进而通过增加机器数据，进行分布式的横向扩展。因此往往需要对大数据进行数据分片，在数据查找时如何确定数据存储的位置便需要数据路由。
### 数据分片：
* 抽象模型：通常分为两级映射:1,key-partition：数据key值到数据分片的映射；2.partition-machine:数据分片到机器的映射；
* 哈希分片:主要通过哈希函数确定数据分片，仅支持点查询，不支持范围查询；如Dynamo,Cassandra,Riak,Voldmort,Membase等大多数KV存储系统
    * Round Robin: 即对key进行取模，hash(key)mod K(K通常为物理机器数量)，由于哈希函数与物理机器数有关，所以系统通常难以扩展。
    * 虚拟桶:所有数据记录通过哈希函数映射到对应的虚拟桶，然后通过查表将虚拟桶映射到物理机(通常为多对一关系)，相比之下更加灵活
    * 一致性哈希:
        * 分布式哈希表DHT是一种技术概念，是常用于P2P(Peer-to-Peer,对等联网)网络和分布式存储，一致性哈希为其一种实现
        * 一致性性哈希算法(Chord,和弦系统中的一致性哈希算法):将哈希数值空间按照大小组成一个首尾相接的环状序列，不同的机器可根据其端口号或Ip等信息映射到环状序列中的不同节点，从而每台机器负责存储维护一段有序哈希空间内的数据，实现数据分片。
            * 路由: 
                * 每个机器配有路由表(通常包含m项，m为哈希空间二进制数值比特位长度，其中第i项路由信息代表距离当前节点为2^i的哈希空间数值所在的机器节点编号),
                * 查找过程如下:
                    * 节点i发起查找，后继为节点s,H(key)=j,若i\<j\<=s说明key对应的数据位于节点s,节点i向s发送消息请求查询key的对应数据值，并返回给节点i，
                    * 否则节点i查询自身的路由表，找到小于j的最大编号n(若均大于j则选择第m-1项路由信息的编号作为n),节点i向节点n发送消息请求查询，此时节点n作为i进行以上步骤，以此循环；
            * 增加新节点:
                * 首先根据想要插入的位置，确定其后继节点，将new节点的后继指向s,前继节点置为空；
                * 然后稳定性检测(所有节点均会周期性自动完成)，完成前驱和后继节点的更新以及数据前移；
                * 此外整个过程的完成还需要等待其他节点的稳定性检测，同时每个节点需要周期性的检查其路由表，对路由表内每一项(k=2^i)加上本机节点编号执行路由算法，得到每个路由项经过查询获得的机器节点，与当前内容不同则更新。
            * 删除已有节点:
                * 正常删除:在离开前通知相应节点更新其前驱和后继节点，以及将本身持有的数据迁移到后继节点上，同时其他节点更新各自路由表
                * 异常删除(往往由于机器故障)，为避免数据丢失可利用数据副本
            * 稳定性检测步骤:
                * 1.节点i向其后继节点s询问其前驱节点p，若后继s的前驱节点p为i本身,则调到步骤4，否则调到步骤2
                * 2.如果节点p介于i和s之间，则i节点记录下p为其后继节点
                * 3.选择i当前的后继节点作为x(可能为s也可能为p),如果x的前驱节点为空，或者i为x和他的前驱节点之间，则i给x发消息指示i就是x的前驱节点，此时x将其前驱节点设为i
                * 4.x(p)节点将部分数据迁移到i节点(即哈希值小于等于i的数据)
            * 虚拟节点:考虑到机器差异，负载均衡等问题，可将一个物理机器虚拟成多个虚拟节点
* 范围分片：通常对key值进行排序，然后划分范围确定数据分片，通常通过映射表(可利用B+树)将数据分片映射到物理机器，对于数据分片在物理机上的管理可利用LSM树。如PNUTS,Azure,Bigtable

## 数据复制与一致性
* 数据复制：为提高系统的容错性和高可用性，往往需要对数据进行冗余存储，同时数据副本可以大大提高数据并发读取的效率。同时数据多副本也会带来并发更新时的一致性维护问题
### 基本原则
* CAP原则(对于同一个系统三者不可兼得，至多只能保证其中两个，应根据实际往往在AP/CP间进行取舍选择)
    * Consistency强一致性:在分布式同一数据多副本的情况下，数据的更新操作随时保持一致(对外如单份数据一样，提供一致性视图)
    * Availability可用性:客户端在任何时刻对大规模数据系统的读写操作应保证在有限时间内完成
    * Partition Tolerance分区容忍性:在大规模分布式数据系统中，网络分区(即分区间机器无法进行网络通信)必然发生，在这种情况下应保证系统可以正常工作
> 注:通常网络分区出现的概率很小，因此在一定程度上我们可以同时保证CA;其次在AC间取舍时,我们可分别考虑中不同的子系统，或针对系统的不同运行时期进行细粒度的策略选择，差异处理；其次CAP原则并非绝对，应将其看作连续变量，可根据实际情况，决定在多大程度上进行保证/舍弃；
* ACID原则(通常数据库系统因此获得高可靠性和强一致性)：
    * Atomicity原子性:即一个事物要么全部执行，要么全部不执行
    * Consistency一致性:在事物的开始和结束总是满足一致性约束条件
    * Isolation事物独立:若多个事物同时执行，彼此之间互不知晓对方的存在，互不影响；即事物之间需要序列化执行
    * Durability持久性:指事物运行成功后对系统状态的更新是永久的，不会无缘故的回滚撤销；
* BASE原则(通常适用于大数据环境下的云存储和NoSql，通过牺牲一致性来获得高可用性)：
    * Basic Available基本可用，允许偶尔失败
    * Soft State软(柔性)状态:指不要求数据状态在任意时刻都要求同步
    * Eventual Consistency最终一致性：符合弱一致性，要求在给定的时间窗口内数据会达到一致状态
> 注:目前NoSql和云存储系统发展过程中正在向逐步提供局部ACID特性发展，同时全局符合BASE原则，发挥两者优势，在两者间建立平衡
* 总结: CAP中的C指强一致性，是ACID中C的一个子集；当网络分区发生时,ACID的事物独立(I)只能在某个分区执行，多个分区会各自进行持久化(D),网络分区解决后需要解决冲突，使整个系统重新达到一致性状态；
* 幂等行性:调用方反复执行同一操作与只正确执行一次操作，效果相同；
### 一致性模型分类
* 强一致性:当某个数据被更新后，其后续所有的观察者应立即感知到其更新值，并以此为基础进行读/写;
* 弱一致性:所有不满足强一致性的情况
    * 最终一致性：在一个时间片段(不一致窗口)后保证所有观察者感知到次更新值
        * 因果一致性:实用与进程之间有因果关系的情形，在某个进程更新了数据值后，会通过消息通知其因果进程数值已改变，从而具有因果关系的进程感知到数据的更新，而对其他进程在不一致窗口内仍无法保证感知到数据的更新；
            * 读你所写一致性:进程更新数据值后保证自身立即感知到数据的更新；
                * 会话一致性:当进程通过会话与数据库连接时，在同一个会话内保证"读你所写"一致性，而在不一致窗口内，若会话终止进程可能无法感知自身的更新操作
        * 单调写一致性:保证在某个进程之后对数据进行读取的其他进程看到的数据版本不能比此进程看到的更老；
        * 单调读一致性:保证多次写操作的序列化
### 副本更新策略
* 同时更新:若不通过一致性协议，对于同一时刻的接收的更新消息无法确定其执行顺序，造成潜在的数据不一致问题；若通过一致性协议预先处理，可确定不同更新操作的执行顺序，保证数据一致性，但一般会增加请求延时
* 主从式更新：副本中存在一个主副本，其他为从副本，所有对这个数据的更新操作首先提交到主副本，再由主副本通知从副本进行数据更新，有主副本决定不同更新操作的顺序
    * 同步方式:主副本等待所有从副本更新完成之后才确认更新操作完成。可保证数据的强一致性，但请求延时较大。
    * 异步方式:主副本在通知从副本更新完成之前即可确认更新操作。可能出现数据一致性问题(一般会在可靠位置保存更新操作记录)
        * 所有读请求都通过主副本来响应(其他副本将读请求转发到主副本处理)，可以保证数据的一致性，但同时也会增加请求延时(如Chubby)
        * 数据的所有副本都可以自己响应读请求，会出现数据的不一致的问题，但同时会降低请求延时(如PNUTS,Zookeeper)
    * 混合方式:主副本首先同步更新部分从副本数据即可确认更新完成，然后其他从副本数据异步更新完成（如Kafka）
        * 若读操作至少从一个同步更新的节点读出，则一致性获得保证,如RWN协议的R+W>N模式，相比同步方式请求延时相对较小,否则会出现读不一致问题，如R+W<N
* 任意节点更新：数据更新请求可能发送给多副本中的任意一个节点，然后由这个节点通知其他节点进行更新。相比"主从式更新"副本中的任意一个节点都可能响应更新请求(因此有可能同一时间对同一数据发出更新请求，但是由不同副本进行响应)，类似于"主从式更新"，对于通知的方式可能有同步/异步两种方式
> 注:Dynamo/Cassandra/Riak采用了主从式的混合方式，当主副本发生故障式则启动任意节点更新策略
### 一致性协议
#### 两阶段提交协议2PC:保证分布式事务中，要么所有参与进程都提交事务，要么都取消事务(如可保证数据更新操作的多副本一致性)。更多用来实现数据更新操作的原子性(如Raft协议),可将参与者分为：唯一的协调者,众多的参与者
    * 表决阶段(Voting)：协调者(初始为INIT状态，当接到系统的commit消息后)向所有参与者发送一个VOTE_REQUEST消息(进入WAIT状态)，当参与者(初始为INIT状态)接收到该消息，向协调者回应VOTE_COMMIT消息(进入READY状态)，表示自己已做好提交准备，否则回应一个VOTE_ABORT消息，表示自己目前尚无提交事务的可能
    * 提交阶段(Commit):协调者收集汇总缩所有参与者的表决信息，如果所有参与者均已准备好提交事务(回应为VOTE_COMMIT)，则协调者决定最终事务可提交，并向所有参与者发送GOLBAL_COMMIT消息，通知参与者进行本地提交；否则如果有任意一个参与者没有准备好(返回VOTE_ABORT)，则协调者决定取消事务，并向所有的参与者发送GLOBAL_ABORT消息，通知其取消事务；然后参与者根据收到的消息进行本地操作
* 如果一个协议包含阻塞态(如2PC中协调者的WAIT状态和参与者的INIT和READY状态)，那么明显是一个脆弱的系统,很可能整个系统处于长时间等待。为解决此问题，提出了以下方法：
    * 超时判断机制(解决协调者WAIT和参与者INIT)：WAIT状态的协调者在指定时间内未能收集全部参与者的回应信息，则向全部参与者多播GLOBAL_ABORT消息；如果INIT状态的参与者在规定时间内为收到协调者的VOTE-request消息，则可以简单的中止本地事务，并向协调者发送VOTE-request消息
    * 参与者互询机制(解决参与者READY):READY状态的参与者在规定时间内未收到消息，可以询问另外的参与者：
        * 若另外的参与者处于COMMIT状态，则等待的参与者可以提交本地事务并进入COMMIT状态
        * 若另外的参与者处于ABORT状态，则等待的参与者可以取消本地事务并进入ABORT状态
        * 若另外的参与者处于INIT状态，此时等待的参与者可取消本地事务并进入ABORT状态
        * 若另外的参与者处于READY状态，则可以继续询问另外的参与者(若所有的参与者均处于READY状态，则只能长时间等待协调者重新启动或故障恢复) 
* 三阶段提交协议(3PC，为解决2PC协议的长时阻塞):将2PC的提交阶段再次细分为预提交和提交两个阶段，通过引入预提交，从而使协调者和参与者:
    * 没有可以既可以直接转换到COMMIT又可以直接到ABORT状态的单独状态；
    * 不存在既不能做出决定，而且可从它直接转换到COMMIT状态的状态(PRECOMMIT已经做出决定)；
#### 向量时钟(Vector Clock)：





## 大数据常用算法与数据结构
### 布隆过滤器BF(Bloom Filter):
* 概述:采用二进制向量数据结构，空间效率极高,常用于判断是一个元素在大规模集合中是否存在(存在误判的问题，但不会漏判)
* 利用长度为m的位数组(初始全部置为0)储存集合信息,同时实用k个相互独立的哈希函数将数据映射到位数组空间(对于数据a,Hi(a)=x,则将位数组的x位置为1，对于每个数据分别利用k个哈希函数进行计算可将位数组的w位置为1，w<=k)，当查询数据b时，实用相同的k个哈希函数进行计算，若对应的位均为1则集合包含该数据，有一个位为0则不包含。
* 误判率主要与集合大小n,哈希函数个数k，位数组大小m有关:P约等于（1-e^-(kn/m)）^k;在n和m确定时k=(m/n)ln2时误判率P达到最低；在集合大小n确定时，m=-(nlnP/(ln2)^2)
 * 改进(满足集合动态变化的需求)：Counting Bloom Filter将位数组的每一位扩展为n位，当Hi(a)=x时数组对应的x位加一(同时在删除一个元素时，映射后对应的位减一)
 * 应用:实用于数据量极大且容忍误判的情况，数据库中的Bloom Jion加速两个大小差异巨大的表的Jion过程；在Bigtable中数据读取也用到BF,此外还有Cassandra,MillWheel
 ### SkipList
 * 一种可代替平衡树的数据结构(大多数情况下可实现O(logn)的增删查改时间复杂度)，依靠随机生成数以一定概率保持数据的平衡分布，适用于有序存储，如LevelDB中的MemTable,Redis中的Sorted Set,Lucene中的倒排列表的查找都用到这种数据结构
 * 为链表中的部分节点增加更多的指针指向其后面的不同距离节点，并且在插入节点时随机决定该节点有多少指向后续节点的指针(称为该节点的Level,表头具有MaxLevel层级)
  * 查找:从首节点的最高层级节点开始查找，若node-key小于search-key，指针后移，若等于则返回结果。若大于则指针移向下一层级，到达尾部还没找到则查找失败
  * 插入:首先类似用类似查找的方法找到要插入的位置(第一个node-key>search-key的的节点前，若找到key相同节点则直接更新该节点值)；然后保存每一层的后继指针；然后生成插入新节点并调整指针；
  * 删除：类似插入，查找要删除的节点，保存指针，删除节点后重新调整指针
### LSM树