---
title: 这就是搜索引擎（核心技术详解）
date: 2017-12-9 16:00
tags: [搜索引擎,信息检索,张俊林,读书笔记,算法]
---


＃ 这就是搜索引擎：核心技术详解(张俊林)


## 搜索引擎及其技术架构
*产生背景: 随着上世纪互联网技术的的快速发展，个人电脑的数量急剧增加，同时WEB技术的出现,internet成为人们获取信息的主要载体，各种web站点从而急剧增加，如何从纷繁的互联网信息中快速有效的获取人们所需要的信息成为一个重要的问题，而搜索引擎的出现恰恰解决了这个问题。同时搜索引擎也成为人们网上冲浪的重要的入口。
* 发展历程:分类目录(人工)　　－》　　文本检索(简单文本匹配)　　－》　　链接分析(排序)　　　－》　　　用户中心(个性化)
* 搜索引擎三个目标:更全，更快，更准；为了这三个目标，引入了索引，索引压缩，排序，链接分析，反作弊，用户研究，云存储，爬虫，网页去重，缓存等技术；
* 搜索引擎的３个核心问题(不同发展时期关心的侧重不同):
    * (分析)用户真正需求:不同用户，不同场景，不同时空，相同的搜索词对应的真正意图不同；
    * (匹配)那些信息和用户相关:信息检索模型，相关性计算，关键词匹配，自然语言理解
    * (过滤)哪些信息用户可信赖：反作弊，结果排序
* 搜索引擎架构(获取存储海量信息/快速响应用户查询):
    * 收集阶段
        * 网络爬虫获取网页信息，经过去重模块去重
        * 搜索引擎进行网页解析，抽出主体内容，将建立的(倒排)索引存储结构，和链接关系保存在云存储平台；
    * 响应阶段:
        * 查询分析对搜索词进行分析，相关性匹配找到相关内容，链接分析等对其进行排序；
        * 此外还有缓存系统用来加快响应的速度
    * 此外还有反作弊系统等用来提高用户体验
## 网络爬虫
* 通用爬虫框架:
    * 待抓取URL队列(初始为种子URL),
    * 网页下载器:根据待抓取URL下载网页，并存入网页库，并额外维护一个已抓取URL队列(避免重复抓取)
    * URL抽取已网页网页中的URL链接信息，放入待抓取URL队列
* 爬虫系统分类
    * 批量型爬虫:有明确抓取范围:如网页数量，抓取时间等
    * 增量型爬虫:持续不断抓取，并定期更新已抓取的网页
    * 垂直型爬虫:关注特定主题内容:需要分析网页主题，进行筛选
* 优秀爬虫特性：高性能，可扩展性，健壮性，友好性(爬虫禁抓协议)
* 爬虫质量评价标准:覆盖率，时新性，网页重要性
* 抓取策略:待抓取URL队列的排序
    * 宽度优先遍历:即将已抓取网页的URL链接依次放入待抓取URL队列的尾部(包含一定的重要性排序，一般较为重要的网页会先被发现)
    * 反完全PageRank策略:根据已抓取的部分网页进行pagerank排序(效果不佳)，通常当新下载的网页达到一定数量，遍历所有的待抓取URL进行pagerank排序，在间隔期间对于新加入的URL赋予一个临时的pagerank值(所有入链的pangerank值累积)
    * OCIP策略(online page importance computation):可视为改进的pagerank算法，初始时每个网页有相同的"现金"，然后抓取后将自身的"现金"均分给所有的出链，自身清空。然后根据URL的“现金”多少进行排序
    * 大站优先策略:根据URL所属网站分类，那个网站等待下载的页面最多则优先
* 网页更新策略:
    * 历史参考策略:参考该网页的历史更新情况，进行泊松过程建模预测
    * 用户体验策略:保存网页多个历史版本，根据过去每次网页内容更新对搜索质量的影响，作为优先更新的参考依据
    * 聚类抽样策略:根据网页的某些属性对网页进行聚类，从每个类别中抽取几个最具代表性网页的并计算其更新周期，并作为此类别所有网页的更新周期(解决新网页的更新周期冷启动问题)
* 暗网抓取:指常规抓取方式无法抓取到的页面(如一些网站的根据用户输入内容(条件)返回的结果页面)
    * 查询组合问题:对于一个结果页面，条件的组合方式可能多种多样
        *　富含信息查询模板:对于某个查询模板的所有属性均赋值，形成不同的查询组合，提交得到结果页面，若结果网页的内容差异性较大，则这是一个富含信息模板；
        * ISIT算法:首先从一维模板开始，逐个分析，然后扩展到二维，逐步增加维度
    * 文本框填写问题:首先根据人工初始化的种子关键词词表，向垂直搜索引擎提交查询，然后从返回的网页中抽取关键词加入词表，以此递归迭代进行
* 分布式爬虫:
    * 可分为不同层级:如分布式数据中心，分布式抓取服务器，分布式爬虫程序，整个爬虫系统由分布式在各地的数据中心组成，每个数据中心负责抓取本区域的网页，每个数据中心又包含多个抓取服务器，每个抓取服务器可部署多个爬虫程序。
    * 主从式分布爬虫:一台主服务器专门对其他从服务器提供URL分发服务(维护待抓取URL队列，并从网页中获取待抓取URL,则外还负责负载均衡等全局服务)，从服务器则负责实际网页抓取下载
    * 对等式分布爬虫:由服务器自己判断某个网页是否应该自己抓取(根据对URL哈希取模，一致性哈希等方法)    
##　搜索引擎索引
* 单词－文档矩阵:横行表示每个文档，纵列标示每个单词，对应的每个交叉的单元格表示每个文档是否哪些单词(或每个单词属于哪些文档)
    * 常见实现方式:倒排索引，签名文件，后缀树等
* 倒排索引:由单词词典和倒排文件组成:
    * 单词词典：由所有的文档集合中出现的所有单词构成的字符串集合(通常用单词编号对单词进行标识)，每条索引项记载单词本身的一些信息和指向该单词对应倒排列表的指针；
        * 数据结构:通常采用哈希加链表，或者B树(B+树)等查找树结构
    * 倒排列表:记载出现过该单词的所有文档的集合(通常用文档编号标识每个文档，对于一个文档集合通常用差值的方式压缩存储)，以及单词出现的频率，单词出现的位置等信息；
        * 对应文档集合内的每个文档称为一个倒排索引项； 所有单词的倒排列表组成一个倒排文件
* 建立索引:
    * 两遍文档遍历:
        * 第一遍:扫描文档集合，收集文档总数N，不同单词的总数M，每个单词在多少文档中出现DF等全局信息，从而确定最终索引项所需要的存储空间；并分配一段连续的存储空间，根据每个单词对应的索引项集合大小需要的将其划分不同区段，每个单词拥有指向自身对应内存片段的起始终止指针；
        * 第二遍:对于每个单词获得包含该单词的所有的文档ID,以及单词在文档中出现的次数TF等信息，填充第一遍遍历分配的该单词对应的内存区段。最终建立完整索引结构，将其保存到磁盘；

    *　排序法:
        * 依次读入文档，对文档进行唯一编号，并对于文档中出现的单词，首先在词典中进行查找获得单词ID(若该单词不存在则将其追加到词典中并分配新的编号)，然后根据文档内容建立，【单词ID，文档ID,单词频率】三元组将其保存到中间结果，依次处理理不同的文档；
        * 当中间结果达到一定大小时，已单词ID为主键,文档ID为次键对所有的三元组进行排序，排序完成后将中间结果写入到磁盘中保存(此时词典一直保留在内存中，并不断增大)，然后释放内存空间后，继续处理后续文档
        * 最后对于磁盘中所有的中间根据相同的单词ID进行合并,生成该单词最终的索引项.(由于单词ID有序，所以利用多个缓冲区高效合并即可)
    * 归并法:
        * 首先依次处理文档，在内存中建立单词词典对应倒排列表的完整倒排索引的数据结构。当占用内存空间达到一定大小后，将其完整的写入到磁盘中，并清空内存，依次处理所有文档；
        * 最后对磁盘中单个不完整倒排索引进行合并。
* 动态索引
    * 在实际搜索引擎系统中，随着文档的更新，通常索引结构也在实时变化。对于动态索引结构通常由三部分构成倒排索引，临时索引，和已删除文档列表；
    * 倒排索引是对初始文档建立的持久化索引结构，通常词典位于内存中，倒排文件放在磁盘中。
    * 临时文件和已删除文件列表反应索引的实时变化，对新加入的文档，对其建立临时索引(完全位于内存中)，对于删除的文档将其加入已删除文档列表(对于更改的文档，视为先删除旧文档，然后加入新文档)
    * 对于一个用户请求，同时在倒排索引和临时索引中进行查找，然后合并两个结果索引项，然后利用已删除文档列表对合并后的结果进行过滤，生成最终的返回结构；
* 索引更新策略:
    *　当临时索引占用的内存空间越来越大，需要将临时索引的内存更新到磁盘索引中已释放内存空间，从而涉及到索引的更新
    * 完全重建策略:
        * 对新加入的文档和原有文档进行合并，然后对所有的文档重新建立索引；
    * 再合并策略:
        * 首先在临时索引(增量索引)和磁盘中倒排索引的建立时，保证索引项按照相同的规则(如字典序)进行排序。
        * 在合并时，同时遍历临时索引和原有的倒排索引进行合并成为一个新倒排索引(合并后追加/追加)；
    * 原地更新策略:
        * 在老的索引初始建立时会在每个索引项的后面，预留一定的空闲空间。对于临时索引中的索引项，在合并时将其追加到对应的预留空间中
        * 若预留的空间不足以容纳临时合并后的索引项，则需要另外开辟一段足够大存储空间，并将合并后的索引项写入新开辟的空间中
    * 混合策略: 
        * 一般会将单词根据不同性质(如单词倒排列表的长度)进行分类，对其索引采取不同的策略
* 查询处理:
    *　一次一文档:首将查询涉及单词的倒排列表读入内存中，然后对其中涉及的文档，依次计算不同文档与查询信息的相似性得分，最后排序，将得分最高的前K个文档作为返回结果；
    * 一次一单词:首先计算一个的单词的倒排列表中涉及的所有的文档的相似性得分(放在哈希表中)。然后处理下一个单词，并不断将同文档的得分累加。最终排序返回前K个作为结果。
    * 跳跃指针:
        * 背景:在计算文档与查询的相似性时，通常需要确定包含所有查询单词的文档，对倒排列表中直接保存文档ID的索引结构，只需要找到找到所用单词的倒排列表，然后计算其交集即可。但对于保存文档ID差值的索引结构，则需要先恢复文档ID
        * 跳跃指针将一个单词的倒排列表，切分为若干个固定大小的数据块进行保存，并且对每个数据块增加元信息(描述此数据块,如开始的文档ID和指向下一个数据块的跳跃指针)
        *　优点:对于压缩后的索引结构，在查询处理时无需全部解压缩，此外无需比较任意两个文档ID
* 多字段索引:
    * 实际中，对于互联网网页，可分为多个字段，如主题，摘要，正文等。不同字段在查询相关性评分中的作用不同
    * 多索引方式:针对不同的字段建立多个索引结构，在处理查询时，合并多个字段的相关性得分。
    * 倒排列表方式:在倒排列表中每个文档对应的索引项中添加一个该单词对应的字段信息
    * 扩展列表方式:为每个字段建立一个列表，记载每个文档的该字段对应的位置信息。在查询时通过单词在文档中的位置信息即可确定其属于那个字段；
* 短语查询:
    * 位置信息索引: 根据单词在某几个单词在同一文档中的位置信息，判断是否构成一个短语；
    * 双词索引:在内存中保存两个词典，首词(第一个)词典拥有指向下词词典(第二个)的指针，下词词典拥有指向倒排列表的指针，在查询处理时，首词下词组合即构成一个短语
    * 短语索引:在词典中加入多词短语，并维护其倒排列表
    * 混合方式:对于不同的短语(如热门高频短语，包含停用词的短语等)建立不同的索引，在查询时，依次在不同的索引结构在匹配
* 分布式索引:
    * 按文档划分:将整个文档切分为不同的子集，每台机器负责某些文档子集的索引建立与维护，在查询处理时，查询分发器将查询信息分发给所有索引服务器，各自负责查询的处理和响应；
    * 按单词划分:将单词词典进行切分，每个服务其负责维护词典的部分子集，根据查询单词，依次交由不同的索引服务器进行处理，并累加相似性得分，最终排序返回前k个结果；
## 索引压缩
* 通过数据压缩算法，对索引进行压缩可以减小索引占用的存储空间，同时在查询处理时减少磁盘的读写数据量，加快查询响应速度
* 词典压缩:
    * 对于采用B树结构的词典，通常存储单词信息，文档频率(DF)，倒排列表指针三项信息，其中后两项占用空间通常较小且固定(通常为各4字节)，通常针对第一项单词信息进行压缩优化
    * 可以将所有的单词存储在一个单独的内存区域，倒排列表中保存指向该单词的指针，此外进一步还可以将词典进行分块，从而相邻多个词典项可以共享一个单词(数据块)指针
* 倒排列表压缩算法:
    * 在倒排列表中对于文档ID以及单词位置，通常采用差值存储，因此倒排列表中通常保存一系列的小的数值，且数字分布极不均衡；
    * 评价索引压缩算法的指标:压缩率，压缩速度，解压速度(与响应速度有关)
    * 基础理论:
        * 一元编码:对于整数X，可利用X-1个二进制数字1和末尾一个数字0进行表示，适合小数值的表示；
        * 二进制编码:即计算机采用的利用二进制数字1和0对数字进行编码
    * Elias Gamma算法:
        * 利用分解函数(X=2^e+d)将待压缩数字分解为两个因子,之后对e+1采用一元编码表示，对d采用比特宽度为e的二进制编码表示
        * Elias Delta算法在Elias Gamma算法进一步改进，对于分解后的因子e+1再次分解（e+1=2^e2+d2）,然后对e2进行一元编码表示；
    * Golomb算法:
        * 与Elias Gamma算法方法类似，但分解函数不同:e1=（X-1）/b;e2=(X-1)mod b ,然后对于e1+1采用一元编码表示，对于e2采用比特宽度为log(b)的二进制编码表示；
        * 假设一个待压缩数字序列的均值为avg,Goloma算法令b=0.69 x avg;
        * RICE算法与Goloma算法仅仅在于b的取值不同，RICE算法要求b必须为尽可能大的2的整数幂，同时小于avg
    * 变长字节压缩算法: 
        * 已字节为单位进行存储，将每个字节分为两部分，其中最后一位用于标识存储元素的边界(为0代表末尾字节)，剩下的前7为用于存储要压缩的数字
        * 以128为基数对待压缩的数字进行编码，一位(0-127)占用一字节的存储空间，利用连续的几个字节进行存储；
    * Simple X算法
        * 为一种字对齐压缩算法，通常以固定字节作为压缩单位；将固定的存储空间分为两个部分，其中前面一小段作为管理信息存储区，剩下的空间作为数据存储区；
        * Simple 9算法，以4字节为一个压缩单位，其中前4个比特为管理信息区，指示后续的数据存储区的存储结构为那种类型(如后续每个数字占用的比特宽度B)，B的值可为1-9,从而将后续的数据存储区分为9中不同的类型；在压缩时对于后续的待压缩数字采用尝试的方法(B=1-9)确定B的数值。
        * Simple 16算法可以将数据存储区划分为16中不同的类型(B=0-15),同时允许数据存储区的不等宽划分(从而解决了simple9的空间浪费)
    * PForDelta算法:
        * 对于待编码的连续K(一般K=128)个数，将其中10%的大数单独存储，同时根据剩下的90%的较小的数字确定存储采用的比特宽度
        * 整个存储结构可划分为三个部分异常链表头+常规数据存储区+异常数据存储区(对于序列中的大数在常规数据存储区中用其在异常数据存储区的位置代替，同时异常链表头保存第一个大数在常规区的位置)
        * 对于异常存储区的大数，对其不做压缩编码，利用4个字节直接存储。顺序与在原序列中的顺序相反；
        * 在压缩时首先，确定异常的大数，和数据存储区采用的比特宽度；然后先处理异常大数，写入异常存续区，并构建异常链表；最后将剩下的数字压缩编码后写入数据存储区
* 文档编号重排序:
    * 倒排列表中文档ID的存储通常采用D-Gap(差值编号)，通过对文档ID重排序，使得倒排列表中相邻的文档ID尽可能的小，从而可以减少占用的空间大小；
    * 可以对网页文档进行文本聚类，然后将内容主题相似的文档依次编号；
* 静态索引裁剪:
    * 在处理查询时，往往并不需要返回与某单词有关的所有的文档。所以可以将那些不那么重要的索引项从倒排列表中删除，从而减少索引项的大小，达到有损压缩的目的；
    * 以单词为中心的索引裁剪:利用相似性函数，对一个单词对应的索引项文档进行相似性计算,并排序。然后根据一定的规则设定裁剪阈值(通常要求至少保留K个文档，且要保证文档有一定的富余)，然后将低于阈值的文档索引项裁剪掉；
    * 以文档为中心的索引裁剪:在索引建立时，首先对文档中的单词进行处理，去除掉那些对于文档的主题不那么重要的单词；

## 检索模型与搜索排序
* 检索模型用来在用户需求已经由查询词明确表征的情况下，找出与之内容相关的文档。
* 布尔模型:
    * 将文档用一系列单词集合来表示，将查询表示一系列由逻辑允许符连接的单词序列。若文档中包含指定单词则为真，最后返回所有满足查询逻辑表达式的文档
* 向量空间模型
    * 将文档D和查询Q由一个t维特征向量进行表示(特征可以为单词，词组，N-gram片段等)，向量中元素可以为通过规则计算出的单词权重
    * 相似性计算：cosine(Q,D)=（Q.D）/(|Q||D|)；(映射到t维空间中为两个向量的夹角)，然后根据相似性排序，返回前K个文档；
    * 特征权重计算(Tf*IDF框架:
        * 词频因子(Tf):即一个单词在文档中出现的次数（一种词频因子变体公式为Tf2=1+log(Tf),或 Tf2=a+(1-a)X（Tf/Max(Tf)）,第二种称为增强型规范化Tf，a为调节因子，通常为0.5,Max(Tf)为在该文档的所有单词的最大的Tf)
        * 逆文档频率因子IDF:为一个文档集合的全局因子，对于给定的文档集合，每个单词的IDF确定，与具体文档无关，考虑特征单词之间相对重要性:IDFk=log(N/nk),其中N为文档集合中的文档总数，nk为单词k在多少个文档中出现
        *Tf\*IDF框架:对于某个特征的对应权重wk=TfXIDFk
* 概率检索模型:
    * 基础原理:直接对用户需求相关性进行概率建模；
        * 将文档集合分为相关文档和不相关文档两类;
        * 给定一个文档D其对应的相关性概率为P(R|D)，不相关性概率为P(NR|D)，若P(R|D)>P(NR|D)则可以断定此文档与用户查询相关；
        * 通过移项化简，可以将上面的相关性判断公式转化为一个二值分类问题:P(D|R)/P(D|NR) > P(NR)/p(R) 对于搜索系统只需关注第一项即可
    * 二元独立模型BIM:
        * 两个假设:
            * 二元假设：对于一个单词只考虑其在文档出现与不出现两种情况；
            * 词汇独立性假设:文档中出现的每个单词之间没有关联，各个单词在文档中出现的概率相互独立；
            * 制定一个查询，对于每个文档可用0,1序列D表示，用Pi代表第i个单词在相关文档集合出现的概率,则P(D|R)=sum(Pi)+sum(1-Pj)，其中Di=1,Dj=0
            * 通过以上模型可得到最终的相关性估算公式:P(D|R)/P(D|NR)=E([pi(1-si)]/[si(1-pi)]),其中Di=1,E表示连续乘积，si表示单词i在不相关文档中出现的概率
    * BM25模型:
        * 在BIM的基础上，考虑单词在查询和文档中的权值:sum{BIM*[[k1+1]fi/[K+fi]]\*[[(k2+1)qfi]/[k2+qfi]} 
            * 公式第二部分代表查询词在文档D中的权值，fi代表单词在文档D中的频率，k1和K为经验参数；
            * 公式第三部分代表查询词自身的权值，qfi代表查询词在用户查询中的频率，k2为经验参数
    * BM25F模型:
        * 将文档划分为不同的组成部分，如meta数据，标题等；不同部分权重不同，分别对待；
        * 相关性判别公式:BIM*fi2/(k1+fi2),   fi2=sum(wkX(fui/Bu)),Bu=((1-bu)+buX(ulu/avulu)) ,其中文档包含u个不同的域，每个域的权重为wk,fui为单词i在第u个域中出现的频率，ulu代表域的实际长度，avulu是在整个文档其中中这个域的平均长度。bu为经验参数
* 语言模型方法:
    * 与由用户查询，确定相关文档的方法不同，语言模型首先对文档进行建模，然后根据模型判断该文档生成模型查询的概率，进行排序
    * 可利用数据平滑的方式解决在建模中的数据稀疏问题（即查询中的单词并不是全都在文档中出现）:即将文档中出现的单词的分布概率向没出现过的单词转移一部分，使得每个单词都有非零的分布式值；
    * 此外除了对每个文档进行建模外，可能还需要建一个北京模型:将将文档集合内的所有文档拼接为一个大的文档，对其建立模型。
    * 加入数据平滑后的建模公式:P(Q|D)=E{(1-y)fqi,D/|D|+y*cqi/|D|}
    * 此外还有其他模型，如HMM隐马尔科夫模型，相关模型，翻译模型等
* 机器学习排序:
    * 人工提供标注的训练数据，由机器通过学习算法(打分函数)自动计算出排序公式，适合多特征处理，通常分为人工标注训练数据，文档特征抽取，学习分类函数；
    * 单文档方法:将单一文档转换为特征向量后，利用学习到的打分函数，进行打分，然后排序
    * 文档对方法:对于两个文档，根据学习出的排序模型，直接对两者进行排序；
    * 文档列表方法:依次选择K个文档，根据学习到的判断函数对其打分，然后系统自动排序
* 检索质量评价标准:
    * 精确率：一次搜索结果相关文档所占的比例
    * 召回率:一次搜索结果中包含的相关文档占整个集合中相关文档的比例
    * P@10指标：更关注排名最靠前的结果指标
    * MAP指标：针对多次查询平均准确率进行评价